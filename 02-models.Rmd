# Models

## General machine learning heuristics
In this section I will describe the different heuristics and concepts which are fundamental in used in machine learning.

### Bias-variance tradeoff


### Hyper parameter tuning
Most machine learning methods have hyperparameters which differ from the model parameters since they are set before model is fitted. These hyperparameters typically specify how fast the learning algorithms learn from data (i.e. learning rate, batch size), model complexity (i.e. number of nodes and hidden layers in neural networks), how much we want to regularize the model (i.e. shrinkage rate) and other parameters which the learning algorithm can't optimize during training That is not to say that those are the only types of parameters, that can be hyperparameters. For example if we have a linear model 
$$
Y = \beta_0 + X\beta +\epsilon
$$
and fix a $\beta_i$ before fitting the rest of the $\beta$s then $\beta_i$ becomes a hyperparameter.

The value we those for our hyperparameters may not be the optimal values and that's where we use hyperparameter tuning comes into play. The idea of hyperparameter tuning is that for each hyperparameter $\gamma_i$, $i=1...k$ we have a set of candidate parameters $\gamma_{ij}$ $j=1...l_i$. For each of the $\prod_{i=1}^kl_i$ combinations in the hyperparameter grid we are then going the run the learning algorithm on data and choose the combination of hyperparameters which gives us the best model performance. In order to choose a model with the best generalization we'll measure the model performance either on our validation/development dataset or using cross-validation. The exhaustive search amongst all combinations of candidate parameters is called grid-search. 

If the hyperparameter grid is too large to make it practically feasible to use grid search another method is to sample (without replacement) the grid for number of hyperparameter combinations which are feasible and carry out the hyperparameter tuning on this reduced partial grid.

### Regularization and "the bet on sparsity"

### Model validation

#### Cross-validation
FIXME:
Cross-validation is a method 

#### Train / dev / test

#### Bayesian Approach
  
#### Bootstrapping

## Neural networks
FIXME: Intro
Neural networks consists of an input layer connected to a sequence of hidden layers of neurons which is then connected to an output layer with one or more output nodes. The input layer takes a feature vector i.e. an observation which we denote this vector by $a^{[0]}$ with entries $a^{[0]}_i,\; i = 1\dots p$ and feeds it forward to the neurons in the first hidden layer. The neurons of the first layer then proces the received information by making a linear transformations of the input vector which we denote $z^{[1]}_i,\; i = 1 \dots p_1$ and then applying an activation function $\sigma(x)$ to the linear transformation. The output from this computation is the vector $a^{[1]}$ with entries $a^{[1]}_i, \; i = 1 \dots p_1$ which is then fed forward to next hidden layer until we reach the output layer $l$ which outputs our prediction $a^{[l]}=\hat{y}$. 
To formalize this let
$$
z^{[i]}_j = W^{[i]}_j a^{[i-1]}+ b^{[i]}_j,\; i=1\dots l,\, j=1\dots p_i \\
a^{[i]}_j = \sigma(z^{[i]}_j),\; i=1\dots l,\, j=1\dots p_i \\
$$
Here $W^{[i]}$ is the $p_{i-1}\times p_i$ dimensional weight matrix for layer $i$. $W^{[i]}$

These calculations can be vectorized such that

$$
z^{[i]}_j = W^{[i]}_j a^{[i-1]}+ b^{[i]}_j,\; i=1\dots l,\, j=1\dots p_i \\
a^{[i]}_j = \sigma(z^{[i]}_j),\; i=1\dots l,\, j=1\dots p_i \\
$$


Looking at a network graph we can visualize this.

```{r, eval=FAKSE}
networkgraph
```


In above example we keep the same activation function throughout the entire network. Usually a appropriate activation function is chosen for the output layer depending out the domain of the target variable $y$. We'll go deeper into detail about different activation functions on later.

In the regression setting we usually have one output neuron, but it is possible to have more than one. In the context of insurance one might have a single neural network for a product with an output neuron for each coverage. Here the heuristic is that within the same product a costumer share a basic risk profile across coverages which the neural network learns and can utilize to predict riskpremiums for all coverages at once.

### Vanilla-NN (NN basics)

#### Activation functions

#### Gradient descent for neural networks

### Regularization

#### Norm regularization

#### Drop-out

#### Early stopping

## Regularized general linear models
Regularized

### LASSO regression

### Elastic net regression

## Boosting

### Regularization

## Meta ensemble methods

### Stacking
